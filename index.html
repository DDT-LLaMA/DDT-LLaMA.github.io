<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> DDT-LLaMA</title>

  <link rel="icon" href="./static/images/icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/icon.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <img src="static/images/llama.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">DDT-LLaMA</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens
          </h2>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">Anonymous</span> -->

           <span class="author-block">
            <a href="None">Kaihang Pan</a><sup style="color:#6fbf73;">1</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Wang Lin</a><sup style="color:#6fbf73;">1</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Zhongqi Yue</a><sup style="color:#ffac33;">2</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Tenglong Ao</a><sup style="color:#ac33ff;">3</sup>,</span>
           <span class="author-block">
             <a href="None">Liyu Jia</a><sup style="color:#ffac33;">2</sup>,</span> 
           <span class="author-block">
            <a href="None">Wei Zhao</a><sup style="color:#748cab;">4</sup>,</span><br>
          <span class="author-block">
            <a href="None">Juncheng Li</a><sup style="color:#6fbf73;">1</sup>,</span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=8e7H3PcAAAAJ&hl=en">Siliang Tang</a><sup style="color:#6fbf73;">1</sup>,</span>
          <span class="author-block">
            <a href="https://scholar.google.com.sg/citations?user=YG0DFyYAAAAJ&hl=en">Hanwang Zhang</a><sup style="color:#ffac33;">2</sup></span>
          </div>
         <div class="is-size-5 publication-authors">
           <span class="author-block"><sup style="color:#6fbf73;">1</sup>Zhejiang University,</span>
           <span class="author-block"><sup style="color:#ffac33">2</sup>Nanyang Technological University,</span>
           <span class="author-block"><sup style="color:#ac33ff">3</sup>Peking University</span>
           <span class="author-block"><sup style="color:#748cab">4</sup>Huawei Singapore Research Center</span><br>
           <span class="author-block"><sup>*</sup>Equal Contribution</span><br>
           <span class="paper-block"><b style="color:#f41c1c">CVPR 2025 Oral</b> </span>
           
        </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://DDT-LLaMA.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://DDT-LLaMA.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Eval.AI Link -->
            </div>

          </div>
          <span class="paper-block2">We are currently working on scaling up the training of our DDT tokenizer and the MLLM. <b style="color:#f41c1c">In the near future, we will release a more powerful version of DDT-LLaMA (We rename it as SelfTok), along with a more detailed technical report. Stay tuned!</b></span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 1vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed <img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT</span> tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to develop <img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><img src="static/images/llama.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT-LLaMA</span>, effectively integrating the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. 
          </p>
         
        </div>        
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
            <span class="mathvista">ddt tokens are recursive</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered m-6">
      
      <div class="column is-full has-text-centered content">
        <!-- <h2 class="title is-3"></h2> -->
        <!-- <embed src="static/images/app-emu3.pdf" width="100%" height="700px"/> -->
        <h2 class="title is-3">1. Progressively Image Decoding</h2>
        <div class="content has-text-justified">
        <p>To illustrate the recursive nature of DDT tokens, we only input a subset of the first t timestep tokens that are autoregressively sampled by DDT-LLaMA, into the decoder for image synthesis (t ranges from 1 to 480). As shown in  the following figure, with the number of tokens increasing, the image's attributes are progressively recovered. Initially, the decoder reconstructs fine details, with coarse-grained information such as contours and color information gradually completed until the full structure of the image is outlined. It demonstrates that <b>DDT tokens recursively disentangle visual attributes ranging from fine-to-coarse and are semantically combined to construct the image</b>.</p>
      </div>
        <img src="static/images/mask.png" alt="arithmetic reasoning" width="90%"/><br>
        <!-- <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/1.png" alt="qs-len" width="77%"/>
              <p></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/2.png" alt="qs-len" width="65%"/>
              <p></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/0.png" alt="qs-len" width="78%"/>
              <p></p>
            </div>
          </div>
        </div> -->
      </div>
    </div>
  </div>
</section>

<footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
<p style="font-size: 14px;">
  This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://physbench.github.io/">PhysBench</a>, licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
  Commons Attribution-ShareAlike 4.0 International License</a>.
</p>
        </div>
      </div>
    </div>
</footer>

</body>
</html>

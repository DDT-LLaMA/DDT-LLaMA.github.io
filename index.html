<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> DDT-LLaMA</title>

  <link rel="icon" href="./static/images/icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/icon.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <img src="static/images/llama.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">DDT-LLaMA</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens
          </h2>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">Anonymous</span> -->

           <span class="author-block">
            <a href="None">Kaihang Pan</a><sup style="color:#6fbf73;">1</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Wang Lin</a><sup style="color:#6fbf73;">1</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Zhongqi Yue</a><sup style="color:#ffac33;">2</sup><sup>*</sup>,</span>
           <span class="author-block">
             <a href="None">Tenglong Ao</a><sup style="color:#ac33ff;">3</sup>,</span>
           <span class="author-block">
             <a href="None">Liyu Jia</a><sup style="color:#ffac33;">2</sup>,</span> 
           <span class="author-block">
            <a href="None">Wei Zhao</a><sup style="color:#748cab;">4</sup>,</span><br>
          <span class="author-block">
            <a href="None">Juncheng Li</a><sup style="color:#6fbf73;">1</sup>,</span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=8e7H3PcAAAAJ&hl=en">Siliang Tang</a><sup style="color:#6fbf73;">1</sup>,</span>
          <span class="author-block">
            <a href="https://scholar.google.com.sg/citations?user=YG0DFyYAAAAJ&hl=en">Hanwang Zhang</a><sup style="color:#ffac33;">2</sup></span>
          </div>
         <div class="is-size-5 publication-authors">
           <span class="author-block"><sup style="color:#6fbf73;">1</sup>Zhejiang University,</span>
           <span class="author-block"><sup style="color:#ffac33">2</sup>Nanyang Technological University,</span>
           <span class="author-block"><sup style="color:#ac33ff">3</sup>Peking University</span>
           <span class="author-block"><sup style="color:#748cab">4</sup>Huawei Singapore Research Center</span><br>
           <span class="author-block"><sup>*</sup>Equal Contribution</span><br>
           <span class="paper-block"><b style="color:#f41c1c">CVPR 2025 Oral</b> </span>
           
        </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://DDT-LLaMA.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://DDT-LLaMA.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Eval.AI Link -->
            </div>

          </div>
          <span class="paper-block2">We are currently working on scaling up the training of our DDT tokenizer and the MLLM. <b style="color:#f41c1c">In the near future (Maybe at the end of April<img src="static/images/hugging.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>), we will release a more powerful version of DDT-LLaMA (We rename it as SelfTok), along with a more detailed technical report. Stay tuned!</b></span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 1vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed <img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT</span> tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to develop <img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><img src="static/images/llama.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT-LLaMA</span>, effectively integrating the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. 
          </p>
         
        </div>        
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
            <span class="mathvista">framework</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered m-6">
      
      <div class="content has-text-justified">
        <img src="static/images/model.png" alt="arithmetic reasoning" width="90%"/><br>
        <p><img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT</span> tokens are <b>RECURISIVE</b>, which is reflected in token intrinsic semantics during tokenizer learning. Unlike spatial tokenizers decoding all tokens at once, each <span class="mathvista">DDT</span> token is related to diffusion timesteps, building on the semantics of preceding tokens to compensate for newly-lost info in the current timestep. From early to late timesteps, an expanding subset of DDT tokens is input to the decoder, with new tokens compensating for newly-lost semantics in each timestep. During diffusion process, fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). So <span class="mathvista">DDT</span> tokens recursively acquire attributes ranging from fine-to-coarse and are semantically combined to construct the image.</p><p>
        Based on such recursive visual tokens, we develop <img src="static/images/icon.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><img src="static/images/llama.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">DDT-LLaMA</span> through autoregressive multimodal pretraining. We find <span class="mathvista">DDT-LLaMA</span> effectively integrates the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. </p>
        <p><b>Q:</b> Residual Tokenizers (such as VAR) design multiple tokens compensate for the information loss in a single quantization operation. What is the difference from Residual Tokenizers? </p>
        <p><b>A:</b> Residual tokenizer aims to approximate the visual feature more precisely in a single quantization operation,  WITHOUT altering the essence of 'spatial tokenizer'. It divides tokens into multi-scales (from lower to higher resolutions) and the reconstructed image visually appears as the <b>mixup</b> of images from each scale.In contrast, DDT tokens are NOT spatial tokens, binding token decoding with diffusion timesteps. From early to late timesteps, an expanding subset of DDT tokens is input to the decoder, with new tokens compensating for newly-lost semantics in each timestep. So tokens recursively acquire attributes ranging from fine-to-coarse and are <b>semantically combined</b> to construct the image.
          Below, we demonstrate examples of counterfactual interpolation using two images to further illustrate that VQGAN tokens and VAR tokens are spatial tokens (similar to cutmix or mixup), while DDT-tokens are semantic tokens.
        <br>
        <br>
        <div style="display: grid; place-items: center;">
        <img src="static/images/inter.png" alt="arithmetic reasoning" width="40%"/><br>
        <img src="static/images/inter1.png" alt="arithmetic reasoning" width="60%"/><br>
        <img src="static/images/inter2.png" alt="arithmetic reasoning" width="70%"/><br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
            <span class="mathvista">ddt tokens are recursive</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered m-6">
      
      <div class="column is-full has-text-centered content">
        <!-- <h2 class="title is-3"></h2> -->
        <!-- <embed src="static/images/app-emu3.pdf" width="100%" height="700px"/> -->
        <h2 class="title is-3">Progressively Image Decoding</h2>
        <div class="content has-text-justified">
        <p>To illustrate the recursive nature of DDT tokens, we only input a subset of the first t timestep tokens that are autoregressively sampled by DDT-LLaMA, into the decoder for image synthesis (t ranges from 1 to 480). As shown in  the following figure, with the number of tokens increasing, the image's attributes are progressively recovered. Initially, the decoder reconstructs fine details, with coarse-grained information such as contours and color information gradually completed until the full structure of the image is outlined. It demonstrates that <b>DDT tokens recursively disentangle visual attributes ranging from fine-to-coarse and are semantically combined to construct the image</b>.</p>
      </div>
      <img src="static/images/mask.png" alt="arithmetic reasoning" width="90%"/><br><br>
      <div class="content has-text-justified">
      <p>To more intuitively illustrate the differences in the token properties of spatial tokens (such as VQGAN tokens) and DDT tokens, we utilize a autoregressive class-conditional pretrained GPT model on DDT tokens to sample a series of images. We then encode the corresponding DDT tokens and VQGAN tokens for each image. For both types of tokens, we feed the corresponding tokenizer decoder with an expanding subset of tokens to decode the images, and present these images in the form of a GIF. It is evident that <b>DDT tokens represent a combination of visual attributes of an image, while VQGAN tokens are a concatenation of different spatial regions of the image</b>.</p></div><br>
      <p><b>DDT tokens</b><img src="static/images/arrow-down.png" style="width:1.0em;vertical-align: middle" alt="Logo"/></p>
      <img src="static/images/ddt_result.gif" alt="arithmetic reasoning" width="75%"/><br><br>
      <p><b>VQGAN tokens</b><img src="static/images/arrow-down.png" style="width:1.0em;vertical-align: middle" alt="Logo"/></p>
      <img src="static/images/vq_result.gif" alt="arithmetic reasoning" width="75%"/><br>

      <div class="column is-full has-text-centered content">
        <!-- <h2 class="title is-3"></h2> -->
        <!-- <embed src="static/images/app-emu3.pdf" width="100%" height="700px"/> -->
        <h2 class="title is-3">Counterfactual Interpolation</h2>
        <div class="content has-text-justified">
        <p>We further conduct counterfactual interpolation with both DDT tokens and VQGAN tokens. Specifically, for the token sequence derived from an image, we replace a subset of tokens with those from another image, while keeping the remaining tokens fixed, with the resulting sequence fed into the decoder for image generation. As shown in the following figure, for VQGAN tokens, counterfactual interpolation actually performs <b>CutMix</b>, where regions from two images are concatenated. In contrast, DDT tokens, with their disentangled representation, ensure that only the attributes captured by the substituted tokens change in the generated counterfactuals, which allows for a <b>seamless semantic blending</b> of the two images.</p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/inter/interpolation.png" alt="qs-len" width="90%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/inter/interpolation_2.png" alt="qs-len" width="90%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/inter/interpolation_3.png" alt="qs-len" width="90%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/inter/interpolation_4.png" alt="qs-len" width="90%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/inter/interpolation_1.png" alt="qs-len" width="90%"/>
            <p></p>
          </div>
        </div>
      </div>
        <!-- <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/1.png" alt="qs-len" width="77%"/>
              <p></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/2.png" alt="qs-len" width="65%"/>
              <p></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/cases/0.png" alt="qs-len" width="78%"/>
              <p></p>
            </div>
          </div>
        </div> -->
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
            <span class="mathvista">qualitative examples of ddt-llama</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered m-6">
      
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">T2I Generation</h2>
        <div class="content has-text-justified">
        <p>DDT-LLaMA can effectively follows various types of instructions, including complex ones such as generating surreal images and multi-condition combined prompts, to generate semantically-consistent images. Of course, there is room for improvement in the aesthetic quality of the images. This limitation stems from the fact that our current tokenizer was only trained on ImageNet with a resolution of 256x256.</p>
      </div>
      <!-- <img src="static/images/t2i.png" alt="arithmetic reasoning" width="90%"/><br> -->

      <div id="results-carousel" class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/t2i/t2i.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/t2i/t2i_1.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>Here we present a direct comparison between DDT-LLaMA and Emu3 on prompts involving counting, color, and position. Though Emu3 generates images with higher aesthetic quality, it struggles to accurately respond to such prompts. In contrast, DDT-LLaMA generates images that <b>better align with the object attributes and the spatial relationships specified in the prompt</b>.</p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_1.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_2.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_3.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_4.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_5.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/emu3/compareemu3_6.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
      </div>

      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Image Editing</h2>
        <div class="content has-text-justified">
        <p>DDT-LLaMA supports a wide spectrum of editing operations including both local change (e.g., removal, replacement) and global change (change time, manipulation). For each case, the output images not only resemble the source image to a high degree but also are coherent with the instruction, demonstrating that DDT-LLaMA achieves <b>a great trade-off between fidelity and editability</b>.</p>
      </div>

      <div id="results-carousel" class="carousel results-carousel">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/edit/edit.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/edit/edit_1.png" alt="qs-len" width="95%"/>
            <p></p>
          </div>
        </div>
      </div>

      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Scaling Law on T2I</h2>
        <div class="content has-text-justified">
          <p>We have also observed preliminary indications of scaling laws in our DDT-based MLLM. we also leverage Gemma2-2b for pre-training. And we further compare performance of T2I generation with two model sizes (2B,8B) at three different training stages (50\%, 75\%, and 100\% of total training tokens), as shown in the following figure. The observed improvements in visual quality align with scaling laws, which suggest that larger transformers trained on more extensive datasets can learn more detailed and fine-grained image distributions.</p>
      </div>

      
      <img src="static/images/scaling.png" alt="arithmetic reasoning" width="90%"/>
      
      <!-- <img src="static/images/compareemu3.png" alt="arithmetic reasoning" width="90%"/><br> -->
    </div>
  </div>
  </div>
</section>

<footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
<p style="font-size: 14px;">
  This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://physbench.github.io/">PhysBench</a>, licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
  Commons Attribution-ShareAlike 4.0 International License</a>.
</p>
        </div>
      </div>
    </div>
</footer>

</body>
</html>
